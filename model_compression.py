{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":289311478,"sourceType":"kernelVersion"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom metrics import BalancedAccuracy\nfrom tensorflow.keras import ops\n\n\nclass Distiller(tf.keras.Model):\n    def __init__(self, student, teacher, **kwargs):\n        super().__init__(**kwargs)\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n        self.acc_metric = BalancedAccuracy()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(self,\n                optimizer,\n                student_loss_fn,\n                distillation_loss_fn,\n                alpha=0.5,\n                temperature=3):\n\n        super().compile(optimizer=optimizer)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def compute_loss(self, x=None, y=None, y_pred=None, \n                     sample_weight=None, allow_empty=False):\n        teacher_pred = self.teacher(x, training=False)\n        student_loss = self.student_loss_fn(y, y_pred)\n\n        distillation_loss = self.distillation_loss_fn(\n            ops.softmax(teacher_pred / self.temperature, axis=1), \n            ops.softmax(y_pred / self.temperature, axis=1),\n        ) * (self.temperature**2)\n        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n        self.loss_tracker.update_state(loss)\n        self.acc_metric.update_state(y, y_pred)\n        return loss\n        \n    @property\n    def metrics(self):\n        return [self.loss_tracker, self.acc_metric]\n\n    def call(self, x):\n        return self.student(x)\n\n\n\ndef train_distiller(teacher_trained, \n                    student_model,\n                    train_data,\n                    val_data,\n                    epochs,\n                    learning_rate,\n                    alpha,\n                    temperature,\n                    batch_size):\n    \"\"\"\n    Train student model with distillation, EarlyStopping and Adam optimizer.\n    \n    Args:\n       teacher_trained: teacher model pre-trained.\n       student_model: lightweight model to be trained.\n       train_data: tuple with training images and labels.\n       val_data: tuple with validation images and labels.\n       epochs: int. Number of epochs to train.\n       learning_rate: float. Optimizer learning rate.\n       alpha: float between 0.0 and 1.0. Controls the percentage of the student_loss and the distillation_loss in the final loss.\n       temperature: float greater than or equal to 1.0. The greater results in a more softer probability distribution.\n       batch_size: int. Number of images in each batch.\n    \n    Returns:\n        model: distilled model\n        history: distillation training history\n   \"\"\"\n\n    X_train, y_train = train_data\n    X_val, y_val = val_data\n    ds_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n    ds_val = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n\n    distiller = Distiller(student= student_model, teacher= teacher_trained)\n    distiller.compile(optimizer= tf.keras.optimizers.Adam(learning_rate= learning_rate),\n                      student_loss_fn= tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n                      distillation_loss_fn= tf.keras.losses.KLDivergence(),\n                      alpha= alpha,\n                      temperature= temperature)\n    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                min_delta=0.001,\n                                                patience=20,\n                                                restore_best_weights=True)\n    \n    history = distiller.fit(ds_train, \n                            epochs= epochs,\n                            callbacks=[callback],\n                            validation_data=ds_val,\n                            batch_size= batch_size)\n    \n    return distiller, history\n\n\ndef quantize_model(model, dtype= tf.float16):\n    \"\"\"\n    Apply quantization and convert to a TFLite model.\n\n    Args:\n        model: Full numerical precision Keras model.\n        dtype: dtype to apply quantization.\n\n    Returns:\n        TFLite model quantized.\n    \"\"\"\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.target_spec.supported_types = [dtype]\n    converter.experimental_enable_resource_variables = True\n    quantized_model = converter.convert()\n    return quantized_model","metadata":{"_uuid":"e426e200-3f3b-48c6-838a-5a03e5507950","_cell_guid":"c094e934-ba38-401b-ba2d-c285ec2b8946","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}