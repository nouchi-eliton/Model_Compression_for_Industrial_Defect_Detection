{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":289311478,"sourceType":"kernelVersion"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom metrics import BalancedAccuracy\nimport numpy as np\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\ndef eval_model_logits(y_true, y_pred_logits):\n    \"\"\"\n    Evaluate using logits. Diplays the confusion matrix and calculates precision, recall, f1-score, and balanced accuracy\n\n    Args:\n        y_true: true labels.\n        y_pred_logits: model predictions logits\n\n    Returns:\n        List containing the metrics.\n    \"\"\"\n    y_pred = tf.argmax(y_pred_logits, axis=-1)\n    report = classification_report(y_true, y_pred, target_names=['Normal', 'Defective'], output_dict=True)\n    balanced_acc = BalancedAccuracy()(y_true, y_pred).numpy().item()\n    disp = ConfusionMatrixDisplay.from_predictions(y_true, y_pred, cmap='Blues', display_labels=['Normal', 'Defective']);\n    plt.show()\n    \n    precision = report['Defective']['precision']\n    recall = report['Defective']['recall']\n    f1 = report['Defective']['f1-score']\n    return [precision, recall, f1, balanced_acc]\n\n\ndef eval_tflite(tflite_model, x, y_true, batch_size):\n    \"\"\"\n    Evaluate tflite model.\n\n    Args:\n        tflite_model: tflite model to be evaluated.\n        x: images to be used in the evaluation.\n        y_true: labels of the images.\n        batch_size: int. Number of images in each batch.\n\n    Returns:\n        List containing the metrics.\n    \"\"\"\n    interpreter = tf.lite.Interpreter(model_content= tflite_model)\n    input_index = interpreter.get_input_details()[0]['index']\n    output_index = interpreter.get_output_details()[0]['index']\n    input_shape = interpreter.get_input_details()[0]['shape']\n    n_images = x.shape[0]\n    if tuple(input_shape[1:]) != x.shape[1:]:\n        temp = []\n        for i in range(0, n_images, batch_size):\n            batch_resized = tf.cast(tf.image.resize_with_pad(x[i: i + batch_size], input_shape[1], input_shape[2]), tf.float32)\n            temp.append(batch_resized)\n        x = tf.concat(temp, axis=0)\n    else:\n        x = tf.cast(x, tf.float32)\n\n    interpreter.resize_tensor_input(input_index, (batch_size,) + x.shape[1:])\n    interpreter.allocate_tensors()\n    y_pred_logits = []\n    for i in range(0, n_images, batch_size):\n        if (i + batch_size) < n_images:\n            batch = x[i : i + batch_size]\n        else:\n            batch = x[i:]\n            pad = np.zeros((batch_size - batch.shape[0], *batch.shape[1:]), dtype=np.float32)\n            batch = np.vstack([batch, pad])\n\n        interpreter.set_tensor(input_index, batch)\n        interpreter.invoke()\n        output = interpreter.get_tensor(output_index)\n        y_pred_logits.append(output)\n\n    y_pred_logits = np.vstack(y_pred_logits)[:n_images]\n    report_test = eval_model_logits(y_true, y_pred_logits)\n\n    return report_test\n\ndef plot_metrics(object_metrics, obj):\n    \"\"\"\n    Plot a scatterplot comparing the size and f1-score in each stage of compression.\n    Plot a lineplot with the evolution of the precision, recall, f1-score and balanced accuracy in each stage of compression.\n\n    Args:\n        object_metrics: dict containing all the metrics of the object.\n        obj: str. Name of the object.\n    \"\"\"\n    index_metrics = ['precision', 'recall', 'f1-score', 'balanced_acc', 'size_mb']\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    fig.suptitle(obj)\n    \n    df_metrics = pd.DataFrame(object_metrics, index= index_metrics).T\n    axes[0].set_title('Size MB x F1_score')\n    scatter = sns.scatterplot(ax= axes[0], data= df_metrics, x= 'size_mb', y= 'f1-score', hue=df_metrics.index)\n    scatter.legend_.set_title('Models')\n    axes[1].set_title('Metrics evolution')\n    sns.lineplot(ax= axes[1], data= df_metrics.iloc[:, :-1]) # remove the size_mb from the plot\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"033f3f77-ddc3-4b79-bdf9-c7ee20279e75","_cell_guid":"2ff1f807-16af-4c4f-a978-c762acc8035c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}